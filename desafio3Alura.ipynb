{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Pipeline Completo de Machine Learning para Previsão de Churn (Versão Google Colab)**\n",
    "\n",
    "Este notebook implementa um pipeline completo para prever a evasão de clientes (churn) de uma empresa de telecomunicações. As etapas incluem carga de dados, pré-processamento, balanceamento de classes, remoção de multicollinearidade e treinamento de modelos de Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 1: Instalação de Bibliotecas (Opcional)**\n",
    "*O Google Colab geralmente já possui estas bibliotecas. Execute esta célula apenas se encontrar um erro de importação.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 2: Importação das Bibliotecas**\n",
    "*Nesta célula, importamos todas as ferramentas necessárias para o projeto.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas básicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "\n",
    "# Importar ferramentas do Scikit-learn e Imbalanced-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Ferramenta para análise de multicollinearidade\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# Ferramenta para upload de arquivo no Google Colab\n",
    "from google.colab import files\n",
    "\n",
    "# Configuração de visualização para pandas e matplotlib\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 3: Carga e Limpeza Inicial dos Dados**\n",
    "* **Ação Necessária:** Execute esta célula. Um botão \"Escolher arquivos\" aparecerá. Clique nele e selecione o arquivo `df_limpo.csv` do seu computador.\n",
    "* **O que acontece:** O código fará o upload do seu arquivo para o ambiente do Colab e, em seguida, o carregará em um DataFrame. As etapas de limpeza e renomeação de colunas serão aplicadas na sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Faz o upload do arquivo para o ambiente do Colab\n",
    "print(\"Por favor, selecione o arquivo 'df_limpo.csv'\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "# 2. Carrega o arquivo em um DataFrame do pandas\n",
    "# O io.BytesIO é usado para ler o arquivo que está em memória\n",
    "file_name = list(uploaded.keys())[0]\n",
    "df = pd.read_csv(io.BytesIO(uploaded[file_name]))\n",
    "\n",
    "print(f\"\\nArquivo '{file_name}' carregado com sucesso!\")\n",
    "\n",
    "# --- 3. Limpeza e Pré-processamento Inicial ---\n",
    "\n",
    "# Padronizar nomes das colunas (substituir '.' por '_')\n",
    "df.columns = df.columns.str.replace('.', '_', regex=False)\n",
    "\n",
    "# Tratar os valores nulos com 0\n",
    "df['total_day'] = df['total_day'].fillna(0)\n",
    "df['account_charges_total'] = df['account_charges_total'].fillna(0)\n",
    "\n",
    "# Remover a coluna de ID, que é irrelevante para o modelo\n",
    "df = df.drop(columns=['customerID'])\n",
    "\n",
    "print(\"\\n--- Visualização dos dados após limpeza inicial ---\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 4: Codificação de Variáveis Categóricas (Encoding)**\n",
    "*Aqui, transformamos colunas de texto (categóricas) em um formato numérico que o modelo possa entender, usando a técnica de One-Hot Encoding.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas categóricas (tipo 'object')\n",
    "colunas_categoricas = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Aplicar One-Hot Encoding\n",
    "df_codificado = pd.get_dummies(df, columns=colunas_categoricas, drop_first=True)\n",
    "\n",
    "# Renomear a coluna alvo para maior clareza\n",
    "df_codificado = df_codificado.rename(columns={'churn_Yes': 'churn_sim'})\n",
    "\n",
    "print(\"--- DataFrame após a codificação das variáveis ---\")\n",
    "df_codificado.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 5: Divisão dos Dados e Balanceamento com SMOTE**\n",
    "*Dividimos os dados em conjuntos de treino e teste. Em seguida, usamos a técnica SMOTE para balancear as classes no conjunto de treino, evitando que o modelo fique viciado na classe majoritária.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Separar as features (x) da variável alvo (y)\n",
    "x = df_codificado.drop(columns=['churn_sim'])\n",
    "y = df_codificado['churn_sim']\n",
    "\n",
    "# 2. Dividir os dados em conjuntos de treino e teste\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 3. Aplicar SMOTE apenas no conjunto de treino\n",
    "smote = SMOTE(random_state=42)\n",
    "x_treino_balanceado, y_treino_balanceado = smote.fit_resample(x_treino, y_treino)\n",
    "\n",
    "print(\"\\n--- Distribuição das classes ---\")\n",
    "print(\"Antes do SMOTE (treino):\")\n",
    "print(y_treino.value_counts())\n",
    "print(\"\\nDepois do SMOTE (treino):\")\n",
    "print(y_treino_balanceado.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 6: Padronização das Features Numéricas (Scaling)**\n",
    "*Colocamos todas as features numéricas na mesma escala. Isso é importante para modelos como a Regressão Logística e melhora a performance geral.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar as colunas numéricas para padronização\n",
    "colunas_numericas = x_treino_balanceado.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "# Criar e treinar o scaler COM OS DADOS DE TREINO\n",
    "scaler = StandardScaler()\n",
    "x_treino_padronizado = scaler.fit_transform(x_treino_balanceado)\n",
    "\n",
    "# Apenas transformar os DADOS DE TESTE com o scaler já treinado\n",
    "x_teste_padronizado = scaler.transform(x_teste)\n",
    "\n",
    "# Recriar os DataFrames com os dados padronizados, mantendo os nomes das colunas\n",
    "x_treino_padronizado = pd.DataFrame(x_treino_padronizado, columns=x_treino_balanceado.columns)\n",
    "x_teste_padronizado = pd.DataFrame(x_teste_padronizado, columns=x_teste.columns)\n",
    "\n",
    "print(\"\\n--- Amostra dos dados de treino após padronização ---\")\n",
    "x_treino_padronizado.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 7: Análise e Remoção de Multicollinearidade com VIF**\n",
    "*Verificamos a redundância entre as features. Features com VIF (Fator de Inflação da Variância) muito alto são removidas para aumentar a estabilidade e a interpretabilidade do modelo.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular o VIF para cada feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"feature\"] = x_treino_padronizado.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(x_treino_padronizado.values, i) for i in range(len(x_treino_padronizado.columns))]\n",
    "\n",
    "print(\"--- Fator de Inflação da Variância (VIF) antes da remoção ---\")\n",
    "print(vif_data.sort_values(by='VIF', ascending=False))\n",
    "\n",
    "# Definir colunas a serem removidas com base no VIF (infinito ou > 10)\n",
    "colunas_para_remover_vif = [\n",
    "    'internet_online_security_No internet service',\n",
    "    'internet_online_backup_No internet service',\n",
    "    'internet_device_protection_No internet service',\n",
    "    'internet_tech_support_No internet service',\n",
    "    'internet_streaming_tv_No internet service',\n",
    "    'internet_streaming_movies_No internet service',\n",
    "    'account_charges_total',\n",
    "    'total_day'\n",
    "]\n",
    "\n",
    "# Criar os conjuntos de dados finais para treinamento, removendo as colunas\n",
    "x_treino_final = x_treino_padronizado.drop(columns=colunas_para_remover_vif, errors='ignore')\n",
    "x_teste_final = x_teste_padronizado.drop(columns=colunas_para_remover_vif, errors='ignore')\n",
    "\n",
    "print(f\"\\nNúmero de features originais: {x_treino_padronizado.shape[1]}\")\n",
    "print(f\"Número de features finais após remoção por VIF: {x_treino_final.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 8: Treinamento e Avaliação dos Modelos**\n",
    "*Finalmente, treinamos e avaliamos dois modelos de classificação com nosso conjunto de dados final.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODELO 1: REGRESSÃO LOGÍSTICA ---\n",
    "print(\"\\n---------------------------------------\")\n",
    "print(\"   Modelo: Regressão Logística\")\n",
    "print(\"---------------------------------------\")\n",
    "modelo_lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "modelo_lr.fit(x_treino_final, y_treino_balanceado)\n",
    "predicoes_lr = modelo_lr.predict(x_teste_final)\n",
    "\n",
    "acuracia_lr = accuracy_score(y_teste, predicoes_lr)\n",
    "relatorio_lr = classification_report(y_teste, predicoes_lr, target_names=['Não Churn', 'Churn'])\n",
    "\n",
    "print(f\"Acurácia: {acuracia_lr:.4f}\")\n",
    "print(\"Relatório de Classificação:\\n\", relatorio_lr)\n",
    "\n",
    "\n",
    "# --- MODELO 2: RANDOM FOREST ---\n",
    "print(\"\\n---------------------------------------\")\n",
    "print(\"   Modelo: Random Forest\")\n",
    "print(\"---------------------------------------\")\n",
    "modelo_rf = RandomForestClassifier(random_state=42, n_estimators=100) # n_estimators é um bom parâmetro para ajustar\n",
    "modelo_rf.fit(x_treino_final, y_treino_balanceado)\n",
    "predicoes_rf = modelo_rf.predict(x_teste_final)\n",
    "\n",
    "acuracia_rf = accuracy_score(y_teste, predicoes_rf)\n",
    "relatorio_rf = classification_report(y_teste, predicoes_rf, target_names=['Não Churn', 'Churn'])\n",
    "\n",
    "print(f\"Acurácia: {acuracia_rf:.4f}\")\n",
    "print(\"Relatório de Classificação:\\n\", relatorio_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 9: Análise de Importância das Features do Melhor Modelo (Random Forest)**\n",
    "*Visualizamos quais fatores o modelo Random Forest considerou mais importantes para prever o churn.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um DataFrame com a importância de cada feature\n",
    "importancia_features = pd.DataFrame({\n",
    "    'Feature': x_treino_final.columns,\n",
    "    'Importancia': modelo_rf.feature_importances_\n",
    "}).sort_values(by='Importancia', ascending=False)\n",
    "\n",
    "print(\"\\n--- Top 10 Features Mais Importantes (Random Forest) ---\")\n",
    "print(importancia_features.head(10))\n",
    "\n",
    "# Gerar o gráfico de barras\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importancia', y='Feature', data=importancia_features, palette='viridis')\n",
    "plt.title('Importância das Features no Modelo Random Forest Final', fontsize=16)\n",
    "plt.xlabel('Nível de Importância', fontsize=12)\n",
    "plt.ylabel('Feature', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Célula 10: Conclusão Estratégica**\n",
    "\n",
    "### Conclusão Estratégica e Próximos Passos\n",
    "\n",
    "**1. Modelo Escolhido:** O modelo **Random Forest** apresentou o melhor equilíbrio entre as métricas, especialmente no **Recall** para a classe \"Churn\". Isso significa que ele é mais eficaz em identificar os clientes que realmente têm a intenção de sair, o que é crucial para ações de retenção. O modelo é robusto e confiável, pois foi treinado em dados balanceados e livres de multicollinearidade.\n",
    "\n",
    "**2. Principais Fatores de Evasão:**\n",
    "A análise de importância das features revelou os seguintes fatores como os mais críticos:\n",
    "- **`customer_tenure` (Tempo de Contrato):** Clientes mais novos são mais propensos a sair.\n",
    "- **`account_charges_monthly` (Cobrança Mensal):** O valor da fatura é um fator decisivo.\n",
    "- **`account_contract_...` (Tipo de Contrato):** Planos mensais representam o maior risco.\n",
    "- **`account_payment_method_Electronic check` (Pagamento com Cheque Eletrônico):** Forte indicador de risco.\n",
    "- **`internet_internet_service_Fiber optic` (Internet Fibra Óptica):** Surpreendentemente, um fator de risco que merece investigação.\n",
    "\n",
    "**3. Recomendações Acionáveis:**\n",
    "- **Fidelização:** Criar campanhas agressivas para migrar clientes de planos mensais para anuais, oferecendo descontos ou benefícios.\n",
    "- **Onboarding de Clientes:** Desenvolver um programa de acompanhamento nos primeiros 3-6 meses para novos clientes, garantindo uma boa experiência inicial.\n",
    "- **Análise de Portfólio:** Realizar uma pesquisa de satisfação com clientes de Fibra Óptica para entender as razões da alta evasão (preço, qualidade, concorrência).\n",
    "- **Modernização de Pagamento:** Incentivar a migração do cheque eletrônico para débito automático com um pequeno bônus na fatura.\n",
    "\n",
    "**Próximos Passos:** O modelo está pronto para ser implementado em um ambiente de produção, onde pode ser integrado ao CRM da empresa para gerar uma pontuação de risco de churn para cada cliente em tempo real, permitindo que as equipes de retenção atuem de forma proativa."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}